{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6226b781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images shape: (60000, 784)\n",
      "Test images shape: (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Normalize the images to have values between -0.5 and 0.5\n",
    "train_images = (train_images / 255) - 0.5\n",
    "test_images = (test_images / 255) - 0.5\n",
    "\n",
    "# Flatten the images from 28x28 pixels to 784-dimensional vectors\n",
    "train_images = train_images.reshape((-1, 784))\n",
    "test_images = test_images.reshape((-1, 784))\n",
    "\n",
    "# Print the shape of datasets to confirm the size\n",
    "print(\"Training images shape:\", train_images.shape)  # Should be (60000, 784 = 28 * 28)\n",
    "print(\"Test images shape:\", test_images.shape)       # Should be (10000, 784 = 28 * 28)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45432f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    # ReLU activation function that zeros out negative values\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "def softmax(x):\n",
    "    # Softmax activation function for the output layer\n",
    "    e_x = np.exp(x - np.max(x, axis=1, keepdims=True))  # Improve numerical stability\n",
    "    return e_x / np.sum(e_x, axis=1, keepdims=True)\n",
    "\n",
    "def deriv_relu(x):\n",
    "    # Derivative of ReLU function for backpropagation\n",
    "    return x > 0\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # Initialize weights and biases\n",
    "        self.bias1 = np.zeros((1, hidden_size))\n",
    "        self.weights1 = np.random.randn(input_size, hidden_size) * np.sqrt(2. / input_size)\n",
    "        self.weights2 = np.random.randn(hidden_size, output_size) * np.sqrt(2. / hidden_size)\n",
    "        self.bias2 = np.zeros((1, output_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the network\n",
    "        self.z1 = np.dot(x, self.weights1) + self.bias1        # First linear step\n",
    "        self.a1 = relu(self.z1)                                # Activation function\n",
    "        self.z2 = np.dot(self.a1, self.weights2) + self.bias2  # Second linear step\n",
    "        self.a2 = softmax(self.z2)                             # Output activation function\n",
    "        return self.a2\n",
    "\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        # Compute the cross-entropy loss\n",
    "        m = y_true.shape[0]\n",
    "        log_likelihood = -np.log(y_pred[range(m), y_true])\n",
    "        loss = np.sum(log_likelihood) / m\n",
    "        return loss\n",
    "\n",
    "    def backprop(self, x, y_true):\n",
    "        # Backpropagation to compute gradients\n",
    "        m = y_true.shape[0]\n",
    "\n",
    "        # Gradients for output layer\n",
    "        delta_z2 = self.a2\n",
    "        delta_z2[range(m), y_true] -= 1  # Derivative of cross-entropy with softmax\n",
    "        delta_z2 /= m\n",
    "        dw2 = np.dot(self.a1.T, delta_z2)\n",
    "        db2 = np.sum(delta_z2, axis=0, keepdims=True)\n",
    "\n",
    "        # Gradients for hidden layer\n",
    "        delta_a1 = np.dot(delta_z2, self.weights2.T)\n",
    "        delta_z1 = delta_a1 * deriv_relu(self.z1)  # Element-wise multiplication\n",
    "        dw1 = np.dot(x.T, delta_z1)\n",
    "        db1 = np.sum(delta_z1, axis=0)\n",
    "\n",
    "        # Update weights and biases\n",
    "        self.weights1 -= learning_rate * dw1\n",
    "        self.bias1 -= learning_rate * db1\n",
    "        self.weights2 -= learning_rate * dw2\n",
    "        self.bias2 -= learning_rate * db2\n",
    "\n",
    "    def predict(self, x):\n",
    "        # Predict labels for given input\n",
    "        a2 = self.forward(x)\n",
    "        return np.argmax(a2, axis=1)  # Return the index of highest probability\n",
    "\n",
    "    def train(self, x, y, epochs, learning_rate):\n",
    "        for epoch in range(epochs):\n",
    "            y_pred = self.forward(x)             # Forward pass\n",
    "            loss = self.compute_loss(y, y_pred)  # Compute loss\n",
    "            self.backprop(x, y)                  # Backpropagation\n",
    "            \n",
    "            if epoch % 5 == 0:  # Print loss and accuracy every 5 epochs\n",
    "                predictions = self.predict(x)\n",
    "                accuracy = np.mean(predictions == y)\n",
    "                print(f\"Epoch: {epoch}, Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5bba93a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 2.6763, Accuracy: 0.0632\n",
      "Epoch: 5, Loss: 2.3949, Accuracy: 0.0891\n",
      "Epoch: 10, Loss: 2.2738, Accuracy: 0.1677\n",
      "Epoch: 15, Loss: 2.1979, Accuracy: 0.2293\n",
      "Epoch: 20, Loss: 2.1367, Accuracy: 0.2811\n",
      "Epoch: 25, Loss: 2.0815, Accuracy: 0.3345\n",
      "Epoch: 30, Loss: 2.0294, Accuracy: 0.3857\n",
      "Epoch: 35, Loss: 1.9794, Accuracy: 0.4357\n",
      "Epoch: 40, Loss: 1.9308, Accuracy: 0.4764\n",
      "Epoch: 45, Loss: 1.8834, Accuracy: 0.5115\n",
      "Epoch: 50, Loss: 1.8371, Accuracy: 0.5422\n",
      "Epoch: 55, Loss: 1.7920, Accuracy: 0.5688\n",
      "Epoch: 60, Loss: 1.7482, Accuracy: 0.5913\n",
      "Epoch: 65, Loss: 1.7058, Accuracy: 0.6115\n",
      "Epoch: 70, Loss: 1.6649, Accuracy: 0.6276\n",
      "Epoch: 75, Loss: 1.6252, Accuracy: 0.6432\n",
      "Epoch: 80, Loss: 1.5869, Accuracy: 0.6561\n",
      "Epoch: 85, Loss: 1.5499, Accuracy: 0.6681\n",
      "Epoch: 90, Loss: 1.5143, Accuracy: 0.6785\n",
      "Epoch: 95, Loss: 1.4800, Accuracy: 0.6892\n",
      "Epoch: 100, Loss: 1.4470, Accuracy: 0.6986\n",
      "Epoch: 105, Loss: 1.4153, Accuracy: 0.7071\n",
      "Epoch: 110, Loss: 1.3849, Accuracy: 0.7151\n",
      "Epoch: 115, Loss: 1.3556, Accuracy: 0.7225\n",
      "Epoch: 120, Loss: 1.3275, Accuracy: 0.7296\n",
      "Epoch: 125, Loss: 1.3005, Accuracy: 0.7354\n",
      "Epoch: 130, Loss: 1.2746, Accuracy: 0.7417\n",
      "Epoch: 135, Loss: 1.2496, Accuracy: 0.7467\n",
      "Epoch: 140, Loss: 1.2257, Accuracy: 0.7516\n",
      "Epoch: 145, Loss: 1.2027, Accuracy: 0.7567\n",
      "Epoch: 150, Loss: 1.1806, Accuracy: 0.7612\n",
      "Epoch: 155, Loss: 1.1593, Accuracy: 0.7657\n",
      "Epoch: 160, Loss: 1.1388, Accuracy: 0.7700\n",
      "Epoch: 165, Loss: 1.1191, Accuracy: 0.7734\n",
      "Epoch: 170, Loss: 1.1002, Accuracy: 0.7772\n",
      "Epoch: 175, Loss: 1.0820, Accuracy: 0.7803\n",
      "Epoch: 180, Loss: 1.0644, Accuracy: 0.7837\n",
      "Epoch: 185, Loss: 1.0476, Accuracy: 0.7866\n",
      "Epoch: 190, Loss: 1.0313, Accuracy: 0.7897\n",
      "Epoch: 195, Loss: 1.0156, Accuracy: 0.7920\n",
      "Epoch: 200, Loss: 1.0005, Accuracy: 0.7945\n",
      "Epoch: 205, Loss: 0.9860, Accuracy: 0.7973\n",
      "Epoch: 210, Loss: 0.9720, Accuracy: 0.7994\n",
      "Epoch: 215, Loss: 0.9584, Accuracy: 0.8016\n",
      "Epoch: 220, Loss: 0.9454, Accuracy: 0.8038\n",
      "Epoch: 225, Loss: 0.9327, Accuracy: 0.8060\n",
      "Epoch: 230, Loss: 0.9206, Accuracy: 0.8078\n",
      "Epoch: 235, Loss: 0.9088, Accuracy: 0.8096\n",
      "Epoch: 240, Loss: 0.8974, Accuracy: 0.8115\n",
      "Epoch: 245, Loss: 0.8864, Accuracy: 0.8133\n",
      "Epoch: 250, Loss: 0.8758, Accuracy: 0.8152\n",
      "Epoch: 255, Loss: 0.8655, Accuracy: 0.8167\n",
      "Epoch: 260, Loss: 0.8555, Accuracy: 0.8179\n",
      "Epoch: 265, Loss: 0.8459, Accuracy: 0.8194\n",
      "Epoch: 270, Loss: 0.8366, Accuracy: 0.8206\n",
      "Epoch: 275, Loss: 0.8275, Accuracy: 0.8220\n",
      "Epoch: 280, Loss: 0.8187, Accuracy: 0.8236\n",
      "Epoch: 285, Loss: 0.8102, Accuracy: 0.8251\n",
      "Epoch: 290, Loss: 0.8019, Accuracy: 0.8267\n",
      "Epoch: 295, Loss: 0.7939, Accuracy: 0.8278\n",
      "Epoch: 300, Loss: 0.7861, Accuracy: 0.8292\n",
      "Epoch: 305, Loss: 0.7786, Accuracy: 0.8304\n",
      "Epoch: 310, Loss: 0.7712, Accuracy: 0.8316\n",
      "Epoch: 315, Loss: 0.7640, Accuracy: 0.8327\n",
      "Epoch: 320, Loss: 0.7571, Accuracy: 0.8339\n",
      "Epoch: 325, Loss: 0.7503, Accuracy: 0.8350\n",
      "Epoch: 330, Loss: 0.7438, Accuracy: 0.8360\n",
      "Epoch: 335, Loss: 0.7374, Accuracy: 0.8368\n",
      "Epoch: 340, Loss: 0.7311, Accuracy: 0.8379\n",
      "Epoch: 345, Loss: 0.7251, Accuracy: 0.8390\n",
      "Epoch: 350, Loss: 0.7192, Accuracy: 0.8401\n",
      "Epoch: 355, Loss: 0.7134, Accuracy: 0.8408\n",
      "Epoch: 360, Loss: 0.7078, Accuracy: 0.8415\n",
      "Epoch: 365, Loss: 0.7023, Accuracy: 0.8423\n",
      "Epoch: 370, Loss: 0.6970, Accuracy: 0.8431\n",
      "Epoch: 375, Loss: 0.6918, Accuracy: 0.8438\n",
      "Epoch: 380, Loss: 0.6868, Accuracy: 0.8444\n",
      "Epoch: 385, Loss: 0.6818, Accuracy: 0.8451\n",
      "Epoch: 390, Loss: 0.6770, Accuracy: 0.8459\n",
      "Epoch: 395, Loss: 0.6723, Accuracy: 0.8466\n",
      "Epoch: 400, Loss: 0.6677, Accuracy: 0.8473\n",
      "Epoch: 405, Loss: 0.6632, Accuracy: 0.8479\n",
      "Epoch: 410, Loss: 0.6588, Accuracy: 0.8486\n",
      "Epoch: 415, Loss: 0.6546, Accuracy: 0.8495\n",
      "Epoch: 420, Loss: 0.6504, Accuracy: 0.8499\n",
      "Epoch: 425, Loss: 0.6463, Accuracy: 0.8504\n",
      "Epoch: 430, Loss: 0.6423, Accuracy: 0.8511\n",
      "Epoch: 435, Loss: 0.6384, Accuracy: 0.8516\n",
      "Epoch: 440, Loss: 0.6346, Accuracy: 0.8522\n",
      "Epoch: 445, Loss: 0.6308, Accuracy: 0.8527\n",
      "Epoch: 450, Loss: 0.6272, Accuracy: 0.8535\n",
      "Epoch: 455, Loss: 0.6236, Accuracy: 0.8540\n",
      "Epoch: 460, Loss: 0.6201, Accuracy: 0.8545\n",
      "Epoch: 465, Loss: 0.6167, Accuracy: 0.8551\n",
      "Epoch: 470, Loss: 0.6133, Accuracy: 0.8557\n",
      "Epoch: 475, Loss: 0.6100, Accuracy: 0.8562\n",
      "Epoch: 480, Loss: 0.6068, Accuracy: 0.8568\n",
      "Epoch: 485, Loss: 0.6036, Accuracy: 0.8572\n",
      "Epoch: 490, Loss: 0.6005, Accuracy: 0.8578\n",
      "Epoch: 495, Loss: 0.5975, Accuracy: 0.8581\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "epochs = 500\n",
    "hidden_size = 128\n",
    "\n",
    "# Initialize the neural network\n",
    "nn = NeuralNetwork(784, hidden_size, 10)\n",
    "\n",
    "# Train the neural network using the training data\n",
    "nn.train(train_images, train_labels, epochs, learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93011188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.8672\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test dataset\n",
    "predictions = nn.predict(test_images)\n",
    "\n",
    "# Calculate accuracy by comparing to the true labels\n",
    "accuracy = np.mean(predictions == test_labels)\n",
    "\n",
    "# Print the accuracy\n",
    "print(\"Test accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f88189",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

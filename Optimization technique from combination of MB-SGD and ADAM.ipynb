{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images shape: (60000, 784)\n",
      "Test images shape: (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Normalize the images to have values between -0.5 and 0.5\n",
    "train_images = (train_images / 255) - 0.5\n",
    "test_images = (test_images / 255) - 0.5\n",
    "\n",
    "# Flatten the images from 28x28 pixels to 784-dimensional vectors\n",
    "train_images = train_images.reshape((-1, 784))\n",
    "test_images = test_images.reshape((-1, 784))\n",
    "\n",
    "# Print the shape of datasets to confirm the size\n",
    "print(\"Training images shape:\", train_images.shape)  # Should be (60000, 784 = 28 * 28)\n",
    "print(\"Test images shape:\", test_images.shape) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The new Algorithm is the combination of Mini-Batch Stochastic Gradient Descent (MB-SGD) and Adaptive Moment Estimation (Adam) optimization technique "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    # ReLU activation function that zeros out negative values\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "def softmax(x):\n",
    "    # Softmax activation function for the output layer\n",
    "    e_x = np.exp(x - np.max(x, axis=1, keepdims=True))  # Improve numerical stability\n",
    "    return e_x / np.sum(e_x, axis=1, keepdims=True)\n",
    "\n",
    "def deriv_relu(x):\n",
    "    # Derivative of ReLU function for backpropagation\n",
    "    return x > 0\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # Initialize weights and biases\n",
    "        self.bias1 = np.zeros((1, hidden_size))\n",
    "        self.weights1 = np.random.randn(input_size, hidden_size) * np.sqrt(2. / input_size)\n",
    "        self.weights2 = np.random.randn(hidden_size, output_size) * np.sqrt(2. / hidden_size)\n",
    "        self.bias2 = np.zeros((1, output_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the network\n",
    "        self.z1 = np.dot(x, self.weights1) + self.bias1        # First linear step\n",
    "        self.a1 = relu(self.z1)                                # Activation function\n",
    "        self.z2 = np.dot(self.a1, self.weights2) + self.bias2  # Second linear step\n",
    "        self.a2 = softmax(self.z2)                             # Output activation function\n",
    "        return self.a2\n",
    "\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        # Compute the cross-entropy loss\n",
    "        m = y_true.shape[0]\n",
    "        log_likelihood = -np.log(y_pred[range(m), y_true])\n",
    "        loss = np.sum(log_likelihood) / m\n",
    "        return loss\n",
    "\n",
    "    def backprop(self, x, y_true):\n",
    "        # Backpropagation to compute gradients\n",
    "        m = y_true.shape[0]\n",
    "\n",
    "        # Gradients for output layer\n",
    "        delta_z2 = self.a2\n",
    "        delta_z2[range(m), y_true] -= 1  # Derivative of cross-entropy with softmax\n",
    "        delta_z2 /= m\n",
    "        dw2 = np.dot(self.a1.T, delta_z2)\n",
    "        db2 = np.sum(delta_z2, axis=0, keepdims=True)\n",
    "\n",
    "        # Gradients for hidden layer\n",
    "        delta_a1 = np.dot(delta_z2, self.weights2.T)\n",
    "        delta_z1 = delta_a1 * deriv_relu(self.z1)  # Element-wise multiplication\n",
    "        dw1 = np.dot(x.T, delta_z1)\n",
    "        db1 = np.sum(delta_z1, axis=0)\n",
    "\n",
    "        # Update weights and biases\n",
    "        self.weights1 -= learning_rate * dw1\n",
    "        self.bias1 -= learning_rate * db1\n",
    "        self.weights2 -= learning_rate * dw2\n",
    "        self.bias2 -= learning_rate * db2\n",
    "\n",
    "    def predict(self, x):\n",
    "        # Predict labels for given input\n",
    "        a2 = self.forward(x)\n",
    "        return np.argmax(a2, axis=1)  # Return the index of highest probability\n",
    "\n",
    "    def train(self, x, y, epochs, learning_rate):\n",
    "        for epoch in range(epochs):\n",
    "            y_pred = self.forward(x)             # Forward pass\n",
    "            loss = self.compute_loss(y, y_pred)  # Compute loss\n",
    "            self.backprop(x, y)                  # Backpropagation\n",
    "            \n",
    "            if epoch % 5 == 0:  # Print loss and accuracy every 5 epochs\n",
    "                predictions = self.predict(x)\n",
    "                accuracy = np.mean(predictions == y)\n",
    "                print(f\"Epoch: {epoch}, Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 2.4128, Accuracy: 0.0824\n",
      "Epoch: 5, Loss: 2.2679, Accuracy: 0.1704\n",
      "Epoch: 10, Loss: 2.1851, Accuracy: 0.2277\n",
      "Epoch: 15, Loss: 2.1204, Accuracy: 0.2784\n",
      "Epoch: 20, Loss: 2.0641, Accuracy: 0.3250\n",
      "Epoch: 25, Loss: 2.0126, Accuracy: 0.3673\n",
      "Epoch: 30, Loss: 1.9639, Accuracy: 0.4051\n",
      "Epoch: 35, Loss: 1.9171, Accuracy: 0.4369\n",
      "Epoch: 40, Loss: 1.8717, Accuracy: 0.4661\n",
      "Epoch: 45, Loss: 1.8278, Accuracy: 0.4929\n",
      "Epoch: 50, Loss: 1.7851, Accuracy: 0.5172\n",
      "Epoch: 55, Loss: 1.7438, Accuracy: 0.5405\n",
      "Epoch: 60, Loss: 1.7039, Accuracy: 0.5617\n",
      "Epoch: 65, Loss: 1.6653, Accuracy: 0.5813\n",
      "Epoch: 70, Loss: 1.6280, Accuracy: 0.5996\n",
      "Epoch: 75, Loss: 1.5921, Accuracy: 0.6151\n",
      "Epoch: 80, Loss: 1.5575, Accuracy: 0.6294\n",
      "Epoch: 85, Loss: 1.5242, Accuracy: 0.6427\n",
      "Epoch: 90, Loss: 1.4922, Accuracy: 0.6545\n",
      "Epoch: 95, Loss: 1.4613, Accuracy: 0.6657\n",
      "Epoch: 100, Loss: 1.4316, Accuracy: 0.6761\n",
      "Epoch: 105, Loss: 1.4031, Accuracy: 0.6848\n",
      "Epoch: 110, Loss: 1.3755, Accuracy: 0.6933\n",
      "Epoch: 115, Loss: 1.3491, Accuracy: 0.7009\n",
      "Epoch: 120, Loss: 1.3236, Accuracy: 0.7078\n",
      "Epoch: 125, Loss: 1.2990, Accuracy: 0.7140\n",
      "Epoch: 130, Loss: 1.2753, Accuracy: 0.7199\n",
      "Epoch: 135, Loss: 1.2525, Accuracy: 0.7260\n",
      "Epoch: 140, Loss: 1.2305, Accuracy: 0.7320\n",
      "Epoch: 145, Loss: 1.2092, Accuracy: 0.7373\n",
      "Epoch: 150, Loss: 1.1887, Accuracy: 0.7427\n",
      "Epoch: 155, Loss: 1.1688, Accuracy: 0.7471\n",
      "Epoch: 160, Loss: 1.1496, Accuracy: 0.7509\n",
      "Epoch: 165, Loss: 1.1310, Accuracy: 0.7551\n",
      "Epoch: 170, Loss: 1.1131, Accuracy: 0.7587\n",
      "Epoch: 175, Loss: 1.0958, Accuracy: 0.7625\n",
      "Epoch: 180, Loss: 1.0792, Accuracy: 0.7655\n",
      "Epoch: 185, Loss: 1.0632, Accuracy: 0.7682\n",
      "Epoch: 190, Loss: 1.0477, Accuracy: 0.7712\n",
      "Epoch: 195, Loss: 1.0328, Accuracy: 0.7742\n",
      "Epoch: 200, Loss: 1.0184, Accuracy: 0.7771\n",
      "Epoch: 205, Loss: 1.0046, Accuracy: 0.7800\n",
      "Epoch: 210, Loss: 0.9911, Accuracy: 0.7822\n",
      "Epoch: 215, Loss: 0.9782, Accuracy: 0.7843\n",
      "Epoch: 220, Loss: 0.9657, Accuracy: 0.7869\n",
      "Epoch: 225, Loss: 0.9535, Accuracy: 0.7891\n",
      "Epoch: 230, Loss: 0.9418, Accuracy: 0.7913\n",
      "Epoch: 235, Loss: 0.9305, Accuracy: 0.7934\n",
      "Epoch: 240, Loss: 0.9195, Accuracy: 0.7952\n",
      "Epoch: 245, Loss: 0.9088, Accuracy: 0.7970\n",
      "Epoch: 250, Loss: 0.8985, Accuracy: 0.7992\n",
      "Epoch: 255, Loss: 0.8885, Accuracy: 0.8008\n",
      "Epoch: 260, Loss: 0.8788, Accuracy: 0.8024\n",
      "Epoch: 265, Loss: 0.8693, Accuracy: 0.8043\n",
      "Epoch: 270, Loss: 0.8602, Accuracy: 0.8060\n",
      "Epoch: 275, Loss: 0.8513, Accuracy: 0.8075\n",
      "Epoch: 280, Loss: 0.8427, Accuracy: 0.8090\n",
      "Epoch: 285, Loss: 0.8344, Accuracy: 0.8107\n",
      "Epoch: 290, Loss: 0.8262, Accuracy: 0.8124\n",
      "Epoch: 295, Loss: 0.8183, Accuracy: 0.8140\n",
      "Epoch: 300, Loss: 0.8107, Accuracy: 0.8153\n",
      "Epoch: 305, Loss: 0.8032, Accuracy: 0.8166\n",
      "Epoch: 310, Loss: 0.7959, Accuracy: 0.8178\n",
      "Epoch: 315, Loss: 0.7889, Accuracy: 0.8190\n",
      "Epoch: 320, Loss: 0.7820, Accuracy: 0.8203\n",
      "Epoch: 325, Loss: 0.7753, Accuracy: 0.8215\n",
      "Epoch: 330, Loss: 0.7688, Accuracy: 0.8227\n",
      "Epoch: 335, Loss: 0.7624, Accuracy: 0.8236\n",
      "Epoch: 340, Loss: 0.7562, Accuracy: 0.8248\n",
      "Epoch: 345, Loss: 0.7502, Accuracy: 0.8259\n",
      "Epoch: 350, Loss: 0.7443, Accuracy: 0.8269\n",
      "Epoch: 355, Loss: 0.7385, Accuracy: 0.8280\n",
      "Epoch: 360, Loss: 0.7329, Accuracy: 0.8290\n",
      "Epoch: 365, Loss: 0.7275, Accuracy: 0.8299\n",
      "Epoch: 370, Loss: 0.7221, Accuracy: 0.8309\n",
      "Epoch: 375, Loss: 0.7169, Accuracy: 0.8318\n",
      "Epoch: 380, Loss: 0.7118, Accuracy: 0.8331\n",
      "Epoch: 385, Loss: 0.7069, Accuracy: 0.8340\n",
      "Epoch: 390, Loss: 0.7020, Accuracy: 0.8347\n",
      "Epoch: 395, Loss: 0.6973, Accuracy: 0.8356\n",
      "Epoch: 400, Loss: 0.6926, Accuracy: 0.8365\n",
      "Epoch: 405, Loss: 0.6881, Accuracy: 0.8374\n",
      "Epoch: 410, Loss: 0.6836, Accuracy: 0.8383\n",
      "Epoch: 415, Loss: 0.6793, Accuracy: 0.8389\n",
      "Epoch: 420, Loss: 0.6750, Accuracy: 0.8396\n",
      "Epoch: 425, Loss: 0.6709, Accuracy: 0.8403\n",
      "Epoch: 430, Loss: 0.6668, Accuracy: 0.8411\n",
      "Epoch: 435, Loss: 0.6628, Accuracy: 0.8415\n",
      "Epoch: 440, Loss: 0.6589, Accuracy: 0.8423\n",
      "Epoch: 445, Loss: 0.6551, Accuracy: 0.8428\n",
      "Epoch: 450, Loss: 0.6513, Accuracy: 0.8433\n",
      "Epoch: 455, Loss: 0.6476, Accuracy: 0.8439\n",
      "Epoch: 460, Loss: 0.6440, Accuracy: 0.8446\n",
      "Epoch: 465, Loss: 0.6405, Accuracy: 0.8455\n",
      "Epoch: 470, Loss: 0.6370, Accuracy: 0.8462\n",
      "Epoch: 475, Loss: 0.6336, Accuracy: 0.8469\n",
      "Epoch: 480, Loss: 0.6303, Accuracy: 0.8476\n",
      "Epoch: 485, Loss: 0.6270, Accuracy: 0.8484\n",
      "Epoch: 490, Loss: 0.6238, Accuracy: 0.8491\n",
      "Epoch: 495, Loss: 0.6207, Accuracy: 0.8497\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "epochs = 500\n",
    "hidden_size = 128\n",
    "\n",
    "# Initialize the neural network\n",
    "nn = NeuralNetwork(784, hidden_size, 10)\n",
    "\n",
    "# Train the neural network using the training data\n",
    "nn.train(train_images, train_labels, epochs, learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.858\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test dataset\n",
    "predictions = nn.predict(test_images)\n",
    "\n",
    "# Calculate accuracy by comparing to the true labels\n",
    "accuracy = np.mean(predictions == test_labels)\n",
    "\n",
    "# Print the accuracy\n",
    "print(\"Test accuracy:\", accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
